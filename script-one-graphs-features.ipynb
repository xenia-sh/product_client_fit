{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Apr 15 21:05:08 2018\n",
    "\n",
    "@author: xshitova\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 16 12:16:55 2018\n",
    "\n",
    "@author: xshitova\n",
    "\"\"\"\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn import datasets, linear_model, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "dataset_test=pd.read_csv('C:/Users/xshitova/Desktop/anon/test_anon.csv', delimiter=';', keep_default_na=False)\n",
    "dataset_train=pd.read_csv('C:/Users/xshitova/Desktop/anon/train1_anon.csv', delimiter=';',keep_default_na=False) \n",
    "\n",
    "cols=[\"feature1\", \"feature2\", \"feature3\", \"feature4\", \"feature5\", \n",
    "      \"feature6\", \"feature7\", \"feature8\",\"feature9\", \n",
    "      \"feature10\", \"feature11\", \"feature12\", \"feature13\"] \n",
    "\n",
    "product_1=[\"PRODUCT ONE\"]\n",
    "product_2=[ \"PRODUCT TWO\"]\n",
    "product_3=[\"PRODUCT THREE\"]\n",
    "product_4=[\"PRODUCT FOUR\"]\n",
    "product_5=[\"PRODUCT FIVE\"]\n",
    "product_6=[\"PRODUCT SIX\"]\n",
    "product_7=[\"PRODUCT SEVEN\"]\n",
    "\n",
    "#-------------------Random tree on each product --------------------\n",
    "print(\"\")\n",
    "\n",
    "dataset_test['feature1'].hist(bins=100)   # plotting histograms of all the features \n",
    "for i in range(0,12):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.yscale('log')\n",
    "    dataset_test[cols[i]].hist(bins=100)\n",
    "    dataset_train[cols[i]].hist(bins=100, ax=ax, label='train', color='blue')\n",
    "    #test and train are shown on the same graph \n",
    "    dataset_test[cols[i]].hist(bins=100, ax=ax, label='test', color='red')\n",
    "    legend = ax.legend() \n",
    "    plt.title(cols[i])\n",
    "    plt.yscale('log') #log scale of the axis \n",
    "    plt.xscale('log')\n",
    "\n",
    "plt.matshow(dataset_train[cols].corr()) #the correlation matrix of features \n",
    "print(dataset_train[cols].corr()) #correlation values \n",
    "\n",
    "#plt.figure(figsize=(12, 16))\n",
    "#t=np.arange(28994)\n",
    "#plt.scatter(X_train[\"feature3\"], X_train['feature10'], c=t) \n",
    "#plt.scatter(X_train[\"feature6\"], X_train[\"feature3\"])\n",
    "#plt.xlabel('feature3')\n",
    "#plt.xticks(X_train['feature5']) \n",
    "#plt.ylabel(\"feature1\")\n",
    "#plt.yticks(X_train['feature1'])\n",
    "#plt.ylim(0,1000)\n",
    "#plt.xlim(0,80)\n",
    "#plt.show()\n",
    "\n",
    "#these are unique values the categorical variables take and which will be turned into separate features \n",
    "feature4_unique=np.array([68,67,64,65,66,98,76,97,83,90,77,0,74,78,95,92,84,91,70])\n",
    "feature2_unique=np.array([15,12,14,13,18,20,0,21,11,10,1,28,19,7,17,4,9,2,8,3,6,5])\n",
    "feature6_unique=np.array([4228,4229,4236,4231,4230,4227,4226,4238,4233,4286,4239,4232,4241,4235,4240,4214,4225])\n",
    "feature5_borders=np.array([192,383,409,506,551,655,771,853,873,950])\n",
    "\n",
    "#this is cycles for creating new features for eachcategory of each categorical variable - here these are variables 2,4,5,6\n",
    "#for i in range(0, len(X_train)):\n",
    "for i in feature4_unique:\n",
    "    cn='feature4_' + str(i)\n",
    "    cols.append(cn)\n",
    "    dataset_train[cn]=np.where(dataset_train['feature4']==i,1,0)\n",
    "    dataset_test[cn]=np.where(dataset_test['feature4']==i,1,0)\n",
    "#    if X_train['feature4'][i]=feature4_unique[0] segment_68='1' else '0'\n",
    "    \n",
    "for i in feature2_unique:\n",
    "    cn='feature2_' + str(i)\n",
    "    cols.append(cn)\n",
    "    dataset_train[cn]=np.where(dataset_train['feature2']==i,1,0)\n",
    "    dataset_test[cn]=np.where(dataset_test['feature2']==i,1,0)\n",
    "    \n",
    "for i in feature6_unique:\n",
    "    cn='feature6_' + str(i)\n",
    "    cols.append(cn)\n",
    "    dataset_train[cn]=np.where(dataset_train['feature6']==i,1,0)\n",
    "    dataset_test[cn]=np.where(dataset_test['feature6']==i,1,0)\n",
    "    \n",
    "j=0\n",
    "prev=0\n",
    "for i in feature5_borders:\n",
    "    j=j+1\n",
    "    cn='newfeature5_' + str(j)\n",
    "    dataset_train[cn]=np.where((dataset_train['feature5']<=i)&(dataset_train['feature5']>=prev),j,0)\n",
    "    dataset_test[cn]=np.where((dataset_test['feature5']<=i)&(dataset_test['feature5']>=prev),j,0)\n",
    "    prev=i\n",
    "\n",
    "dataset_train['newfeature5']=dataset_train['newfeature5_1']\n",
    "dataset_train.drop(['newfeature5_1'], axis=1, inplace=True)\n",
    "dataset_test['newfeature5']=dataset_test['newfeature5_1']\n",
    "dataset_test.drop(['newfeature5_1'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(2,j+1):\n",
    "    cn='newfeature5_' + str(i)  \n",
    "    dataset_train['newfeature5'] = dataset_train['newfeature5'] + dataset_train[cn]\n",
    "    dataset_test['newfeature5'] = dataset_test['newfeature5'] + dataset_test[cn]\n",
    "    dataset_train.drop([cn], axis=1, inplace=True)\n",
    "    dataset_test.drop([cn], axis=1, inplace=True)\n",
    "\n",
    "#remove them as they've been categorized \n",
    "cols.append('newfeature5')\n",
    "cols.remove('feature4')\n",
    "cols.remove('feature1')\n",
    "cols.remove('feature5')\n",
    "cols.remove('feature11')\n",
    "cols.remove('feature6')\n",
    "cols.remove('feature2')\n",
    "\n",
    "X_train=dataset_train[cols] #the explanatory variables train for 2016 - which sorts of clients bought these 7 products \n",
    "Y_train_product_2=dataset_train[product_2]\n",
    "Y_train_product_3=dataset_train[product_3]\n",
    "Y_train_product_5=dataset_train[product_5]\n",
    "Y_train_product_6=dataset_train[product_6]\n",
    "Y_train_product_7=dataset_train[product_7]\n",
    "\n",
    "X_test=dataset_test[cols] #the explanatory variables test for 2017 - which sorts of clients are likely to buy these sorts of products \n",
    "Y_real_product_2=dataset_test[product_2]\n",
    "Y_real_product_3=dataset_test[product_3]\n",
    "Y_real_product_5=dataset_test[product_5]\n",
    "Y_real_product_6=dataset_test[product_6]\n",
    "Y_real_product_7=dataset_test[product_7]\n",
    "\n",
    "\n",
    "#-------------------Random tree on each product --------------------\n",
    "print(\"\")\n",
    "print(\"Random forest results on each one... 66 columns\")\n",
    "n3=200 #this is a variable the value os which can be changed or removed altogether - the number of estimators in a random forest model \n",
    "\n",
    "random_forest2 = RandomForestClassifier(n_estimators=n3)\n",
    "random_forest2.fit(X_train, Y_train_product_2.values.ravel())\n",
    "Y_pred_randomforest_2 = random_forest2.predict(X_test)\n",
    "print(\"product 2 - \" + str(metrics.accuracy_score(Y_real_product_2, Y_pred_randomforest_2)) )#model score product 2 \n",
    "\n",
    "random_forest3 = RandomForestClassifier(n_estimators=n3)\n",
    "random_forest3.fit(X_train, Y_train_product_3.values.ravel())\n",
    "Y_pred_randomforest_3 = random_forest3.predict(X_test)\n",
    "print(\"product 3 - \"+str(metrics.accuracy_score(Y_real_product_3, Y_pred_randomforest_3))) #model score (so forth)\n",
    "\n",
    "random_forest5 = RandomForestClassifier(n_estimators=n3)\n",
    "random_forest5.fit(X_train, Y_train_product_5.values.ravel())\n",
    "Y_pred_randomforest_5 = random_forest5.predict(X_test)\n",
    "print(\"product 5 - \"+str(metrics.accuracy_score(Y_real_product_5, Y_pred_randomforest_5))) #model score \n",
    "\n",
    "random_forest6 = RandomForestClassifier(n_estimators=n3)\n",
    "random_forest6.fit(X_train, Y_train_product_6.values.ravel())\n",
    "Y_pred_randomforest_6 = random_forest6.predict(X_test)\n",
    "print(\"product 6 - \"+str(metrics.accuracy_score(Y_real_product_6, Y_pred_randomforest_6))) #model score \n",
    "\n",
    "random_forest7 = RandomForestClassifier(n_estimators=n3)\n",
    "random_forest7.fit(X_train, Y_train_product_7.values.ravel())\n",
    "Y_pred_randomforest_7= random_forest7.predict(X_test)\n",
    "print(\"product 7 - \"+str(metrics.accuracy_score(Y_real_product_7, Y_pred_randomforest_7)) )#model score \n",
    "\n",
    "print(\"logistic regression...66 columns\")\n",
    "\n",
    "logreg6 = LogisticRegression()\n",
    "logreg6.fit(X_train, Y_train_product_6.values.ravel())\n",
    "Y_pred_logreg_6 = logreg6.predict(X_test)\n",
    "print(\"product 6 - \"+str(metrics.accuracy_score(Y_real_product_6, Y_pred_logreg_6)) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
